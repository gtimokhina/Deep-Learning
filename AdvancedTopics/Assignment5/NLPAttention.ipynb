{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLPAttention.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyP664/kt7ZNu/hAbmW4h4XN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_v_jnD0zrWVO"},"source":["#**Text classification with attention and transformer layer**\n","reference: https://keras.io/examples/nlp/text_classification_with_transformer/"]},{"cell_type":"code","metadata":{"id":"FDeOF85XrTjh","executionInfo":{"status":"ok","timestamp":1605742124906,"user_tz":480,"elapsed":2089,"user":{"displayName":"Gulya Timokhina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFDpXfSY0UztVZChBHTSs4k2cqPlArEcI73d_M9w=s64","userId":"04443340486709856613"}}},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lJxWiX_S1FJw"},"source":["Multi Head self attention as a Keras layer"]},{"cell_type":"code","metadata":{"id":"PYJ1hDzM1C7F","executionInfo":{"status":"ok","timestamp":1605745198579,"user_tz":480,"elapsed":521,"user":{"displayName":"Gulya Timokhina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFDpXfSY0UztVZChBHTSs4k2cqPlArEcI73d_M9w=s64","userId":"04443340486709856613"}}},"source":["class MultiHeadSelfAttention(layers.Layer):\n","    def __init__(self, embed_dim, num_heads=8):\n","        super(MultiHeadSelfAttention, self).__init__()\n","        self.embed_dim = embed_dim\n","        self.num_heads = num_heads\n","        if embed_dim % num_heads != 0:\n","            raise ValueError(\n","                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n","            )\n","        self.projection_dim = embed_dim // num_heads\n","        self.query_dense = layers.Dense(embed_dim)\n","        self.key_dense = layers.Dense(embed_dim)\n","        self.value_dense = layers.Dense(embed_dim)\n","        self.combine_heads = layers.Dense(embed_dim)\n","\n","    def attention(self, query, key, value):\n","        score = tf.matmul(query, key, transpose_b=True)\n","        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n","        scaled_score = score / tf.math.sqrt(dim_key)\n","        weights = tf.nn.softmax(scaled_score, axis=-1)\n","        output = tf.matmul(weights, value)\n","        return output, weights\n","\n","    def separate_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, inputs):\n","        batch_size = tf.shape(inputs)[0]\n","        query = self.query_dense(inputs)  \n","        key = self.key_dense(inputs) \n","        value = self.value_dense(inputs)  \n","        query = self.separate_heads(query, batch_size)  \n","        key = self.separate_heads(key, batch_size)\n","        value = self.separate_heads(value, batch_size) \n","        attention, weights = self.attention(query, key, value)\n","        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  \n","        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))  \n","        output = self.combine_heads(concat_attention)  \n","        return output\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ahr9lF-P388F"},"source":["Layer of Transformer block"]},{"cell_type":"code","metadata":{"id":"SBhsFYog30M3","executionInfo":{"status":"ok","timestamp":1605745403975,"user_tz":480,"elapsed":266,"user":{"displayName":"Gulya Timokhina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFDpXfSY0UztVZChBHTSs4k2cqPlArEcI73d_M9w=s64","userId":"04443340486709856613"}}},"source":["class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XjCumsns4rQT"},"source":["Embedding layer: one for tokens, one for token index"]},{"cell_type":"code","metadata":{"id":"ZOSexaln4lyM","executionInfo":{"status":"ok","timestamp":1605745583729,"user_tz":480,"elapsed":500,"user":{"displayName":"Gulya Timokhina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFDpXfSY0UztVZChBHTSs4k2cqPlArEcI73d_M9w=s64","userId":"04443340486709856613"}}},"source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super(TokenAndPositionEmbedding, self).__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = tf.shape(x)[-1]\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KC4jgRgk5aYS"},"source":["Load and split dataSet"]},{"cell_type":"code","metadata":{"id":"E4C8BnRw5n4g","executionInfo":{"status":"ok","timestamp":1605745774615,"user_tz":480,"elapsed":473,"user":{"displayName":"Gulya Timokhina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFDpXfSY0UztVZChBHTSs4k2cqPlArEcI73d_M9w=s64","userId":"04443340486709856613"}}},"source":["vocab_size = 30000  # take only top 20k words\n","maxlen = 300  # take first 200 words of each movie review"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U2YuoCKT5SP_","executionInfo":{"status":"ok","timestamp":1605745784670,"user_tz":480,"elapsed":7927,"user":{"displayName":"Gulya Timokhina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFDpXfSY0UztVZChBHTSs4k2cqPlArEcI73d_M9w=s64","userId":"04443340486709856613"}},"outputId":"bacdc341-edcf-4a0d-ad07-ba4739324d85"},"source":["(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n","print(len(x_train), \"Training length\")\n","print(len(x_val), \"Validation length\")\n","# padding to maxlen each review\n","x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 1s 0us/step\n","25000 Training length\n","25000 Validation length\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"55NLrfft6XE8"},"source":["Classify text using Transformer Layer"]},{"cell_type":"markdown","metadata":{"id":"ZgaqAB5N7vR_"},"source":["Transformer layer returns one vector for each time step of the input sequence. Calculate the mean across all time steps and do a feed forward network on top to classify text."]},{"cell_type":"code","metadata":{"id":"EkCw7vkI5YJP","executionInfo":{"status":"ok","timestamp":1605746497454,"user_tz":480,"elapsed":494,"user":{"displayName":"Gulya Timokhina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFDpXfSY0UztVZChBHTSs4k2cqPlArEcI73d_M9w=s64","userId":"04443340486709856613"}}},"source":["def create_model(embed_dim, num_heads, ff_dim):\n","  inputs = layers.Input(shape=(maxlen,))\n","  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n","  x = embedding_layer(inputs)\n","  transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","  x = transformer_block(x)\n","  x = layers.GlobalAveragePooling1D()(x)\n","  x = layers.Dropout(0.1)(x)\n","  x = layers.Dense(20, activation=\"relu\")(x)\n","  x = layers.Dropout(0.1)(x)\n","  outputs = layers.Dense(2, activation=\"softmax\")(x)\n","\n","  return keras.Model(inputs=inputs, outputs=outputs)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"MaE58x-X8qXV","executionInfo":{"status":"ok","timestamp":1605746515246,"user_tz":480,"elapsed":4554,"user":{"displayName":"Gulya Timokhina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFDpXfSY0UztVZChBHTSs4k2cqPlArEcI73d_M9w=s64","userId":"04443340486709856613"}}},"source":["embed_dim = 32  # embedding size for each token\n","num_heads = 2  # number of attention heads\n","ff_dim = 32  # hidden layer size in feed forward network inside transformer\n","\n","model = create_model(embed_dim, num_heads, ff_dim)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54Vryvi48y6T","executionInfo":{"status":"ok","timestamp":1605746573446,"user_tz":480,"elapsed":30218,"user":{"displayName":"Gulya Timokhina","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFDpXfSY0UztVZChBHTSs4k2cqPlArEcI73d_M9w=s64","userId":"04443340486709856613"}},"outputId":"b9017120-e935-479d-d2a9-674fdf187a1a"},"source":["model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","history = model.fit(\n","    x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val)\n",")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n","782/782 [==============================] - 14s 17ms/step - loss: 0.3911 - accuracy: 0.8130 - val_loss: 0.2784 - val_accuracy: 0.8795\n","Epoch 2/2\n","782/782 [==============================] - 13s 17ms/step - loss: 0.1880 - accuracy: 0.9294 - val_loss: 0.2942 - val_accuracy: 0.8783\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u-kps8Gg88oU"},"source":[""],"execution_count":null,"outputs":[]}]}